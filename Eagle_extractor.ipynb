{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Processing image batches: 100%|██████████| 333/333 [01:38<00:00,  3.37it/s]\n",
      "Processing image batches: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "Processing image batches: 100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "Processing image batches: 100%|██████████| 1/1 [00:00<00:00, 500.16it/s]\n",
      "Processing image batches: 100%|██████████| 1/1 [00:00<00:00, 994.15it/s]\n",
      "Processing image batches: 100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "Processing image batches: 100%|██████████| 1/1 [00:00<00:00, 663.87it/s]\n",
      "Processing image batches: 100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "Processing image batches: 100%|██████████| 2/2 [00:00<00:00, 1999.19it/s]\n",
      "Processing image batches: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "Processing image batches: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for augmentation ('prepend', 'ohwx artstyle, ', 1): 125.26 seconds\n",
      "Total processing time: 168.31 seconds\n",
      "Processing completed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import zipfile\n",
    "import json\n",
    "from PIL import Image,ImageFile\n",
    "import math\n",
    "global COUNTER\n",
    "import cairosvg\n",
    "import io\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from realesrgan_ncnn_py import Realesrgan\n",
    "import multiprocessing\n",
    "import logging\n",
    "from multiprocessing import Manager\n",
    "from multiprocessing import Lock\n",
    "from PIL import Image, ImageFile,ImageFilter  # Ensure this import is at the top\n",
    "from utils.LLM_API import AsyncLLMProcessor\n",
    "from utils.caption_utils import write_tags_file\n",
    "from utils.image_utils import resize_and_crop_to_fit,fill_transparent_with_color,center_crop_square,svg_scaling\n",
    "from utils.io_utils import ensure_directory_exists\n",
    "import nltk\n",
    "from utils.process_image_API import ProcessImageAPI\n",
    "import time  # Import time module for timing\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "#multiprocessing.log_to_stderr(logging.DEBUG)\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = False\n",
    "target_resolutions = [\n",
    "    (1152, 896), (896, 1152), (1216, 832), (832, 1216),\n",
    "    (1344, 768), (768, 1344), (1536, 640), (640, 1536),(1024, 1024)\n",
    "]\n",
    "\n",
    "\n",
    "def create_tags_file(annotation, tags, folder_path, output_path, augment=None):\n",
    "    output_extension = '.txt'\n",
    "    tags_file_name = f\"{output_path}{output_extension}\"\n",
    "    tags_file_path = os.path.join(folder_path, tags_file_name)\n",
    "    \n",
    "    ensure_directory_exists(os.path.dirname(tags_file_path))\n",
    "\n",
    "    if os.path.exists(tags_file_path):\n",
    "        return\n",
    "    \n",
    "    if use_LLM:\n",
    "        llm_processor.add_to_queue(folder_path, output_path, annotation, tags, augment)\n",
    "    else:\n",
    "        # Existing non-LLM processing logic\n",
    "        from utils.caption_utils import prepare_content, remove_duplicate_phrases\n",
    "        content = prepare_content(annotation, tags, add_tags, shuffle_content, augment=augment)\n",
    "        content = remove_duplicate_phrases(content)\n",
    "        write_tags_file(output_path=tags_file_path, content_list=[content])\n",
    "\n",
    "def process_image_batch(api_instance, images_batch, folder_path, basefolder, image_paths_batch, counter, lock, augment=None):\n",
    "    api_instance.process_images(images_batch, folder_path, basefolder, image_paths_batch, counter, lock, augment=augment)\n",
    "\n",
    "  \n",
    "def make_folders_recursively(root, folder, images, basefolder, processed_dir, counter, lock, augment=None):\n",
    "    safe_folder_name = folder[\"name\"].replace(\"/\", \"_\")\n",
    "    current_path = os.path.join(root, safe_folder_name)\n",
    "    ensure_directory_exists(current_path)\n",
    "\n",
    "    images_in_folder = [image for image in images if folder[\"id\"] in image[\"folders\"]]\n",
    "\n",
    "    batch_size = 10  # Adjust batch size as needed\n",
    "    image_processing_tasks = []\n",
    "    api_instance = ProcessImageAPI({\n",
    "        'pixelart': pixelart,\n",
    "        'append_filename_to_captions': append_filename_to_captions,\n",
    "        'doBucketing': doBucketing,\n",
    "        'isEsganUpscale': isEsganUpscale,\n",
    "        'usePilSave': usePilSave,\n",
    "        'do_center_square_crop': do_center_square_crop,\n",
    "        'padding': padding,\n",
    "        'add_tags': add_tags,\n",
    "        'shuffle_content': shuffle_content,\n",
    "        'use_LLM': use_LLM,\n",
    "        'target_resolutions': target_resolutions\n",
    "    })\n",
    "\n",
    "    for i in range(0, len(images_in_folder), batch_size):\n",
    "        images_batch = images_in_folder[i:i + batch_size]\n",
    "        image_paths_batch = [\n",
    "            os.path.join(processed_dir, f\"{image['id']}.info\", f\"{image['name']}.{image['ext']}\") for image in images_batch\n",
    "        ]\n",
    "\n",
    "        # Add the task to process the batch\n",
    "        image_processing_tasks.append(\n",
    "            delayed(process_image_batch)(api_instance, images_batch, current_path, basefolder, image_paths_batch, counter, lock, augment)\n",
    "        )\n",
    "\n",
    "    if image_processing_tasks:\n",
    "        Parallel(n_jobs=number_of_jobs)(\n",
    "            tqdm(image_processing_tasks, desc=\"Processing image batches\")\n",
    "        )\n",
    "\n",
    "    for child in folder.get(\"children\", []):\n",
    "        make_folders_recursively(current_path, child, images, basefolder, processed_dir, counter, lock, augment=augment)\n",
    "\n",
    "    api_instance.close()  # Ensure any resources are properly released\n",
    "    \n",
    "    \n",
    "def extract_EaglePack_and_process(eaglepacks_path):\n",
    "    processed_dir = os.path.join(eaglepacks_path, \"processed\")\n",
    "    basefolder = os.path.join(eaglepacks_path, \"base\")\n",
    "    \n",
    "    #Unzip all the eaglepacks\n",
    "    ensure_directory_exists(processed_dir)\n",
    "    for filename in os.listdir(eaglepacks_path):\n",
    "        if filename.endswith(\".eaglepack\") and zipfile.is_zipfile(os.path.join(eaglepacks_path, filename)):\n",
    "            with zipfile.ZipFile(os.path.join(eaglepacks_path, filename), 'r') as zipObj:\n",
    "                zipObj.extractall(processed_dir)\n",
    "\n",
    "    #Json to folders\n",
    "    json_file = os.path.join(processed_dir, \"pack.json\")\n",
    "    if os.path.exists(json_file):\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        images = data[\"images\"]\n",
    "        folders = data[\"folder\"]\n",
    "        \n",
    "        ensure_directory_exists(basefolder)\n",
    "        with Manager() as manager:\n",
    "            counter = manager.Value('i', 0)  # Shared counter for parallel processes\n",
    "            lock = manager.Lock()  # Lock to prevent race conditions\n",
    "\n",
    "            for augment in augment_list:\n",
    "                start_time = time.time()  # Start timer\n",
    "                if augment is None:\n",
    "                    # No augmentation\n",
    "                    make_folders_recursively(processed_dir, folders, images, basefolder, processed_dir, counter, lock)\n",
    "                else:\n",
    "                    aug_type, aug_value, aug_percentage = augment\n",
    "                    # With augmentation\n",
    "                    make_folders_recursively(\n",
    "                        processed_dir, folders, images, basefolder, processed_dir, counter, lock,\n",
    "                        augment=(aug_type, aug_value, aug_percentage)\n",
    "                    )           \n",
    "                end_time = time.time()  # End timer\n",
    "                print(f\"Time taken for augmentation {augment}: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Global variables\n",
    "pixelart = False\n",
    "append_filename_to_captions = True\n",
    "doBucketing=True\n",
    "isEsganUpscale=False\n",
    "usePilSave=False\n",
    "number_of_jobs=16\n",
    "do_center_square_crop=False\n",
    "padding = random.randint(80, 100)\n",
    "add_tags = False\n",
    "shuffle_content = False\n",
    "use_LLM=False\n",
    "#target_resolutions = [(768, 768),]\n",
    "\n",
    "if use_LLM:\n",
    "    llm_processor = AsyncLLMProcessor(add_tags=True, shuffle_content=False)\n",
    "    number_of_jobs=1\n",
    "\n",
    "#('tokenOnly', '@regular_icon', 0.4),\n",
    "#('random_dropout_keep_1', None, 1),\n",
    "\n",
    "augment_list = [    \n",
    "#   None,\n",
    "#    ('random_dropout_keep_4', None, 0.7),\n",
    "#    ('prepend', 'ohwx artstyle, ', 1),\n",
    "    ('prepend', 'ohwx artstyle, ', 1),\n",
    "#    ('append', ' ,regular_icon', 0.8)\n",
    "    # None,  # First pass: No augmentation\n",
    "    # ('tokenOnly', '@Bold_icon', 1),\n",
    "    # ('tokenOnly', '@bold_icon', 0.2),\n",
    "    # ('tokenOnly', 'Simple Icon', 0.2),\n",
    "    # ('tokenOnly', 'regular icon', 0.2),\n",
    "    # ('random_dropout_keep_1', None, 0.7),\n",
    "    # ('random_dropout_keep_2', None, 0.7),\n",
    "    # ('random_dropout_keep_3', None, 0.7),\n",
    "    # ('random_dropout_keep_4', None, 0.7),\n",
    "    # ('prepend', 'start_', 1),  # Prepend augmentation\n",
    "    # ('append', '_end', 1)      # Append augmentation\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        start_time = time.time()  # Start timer for entire process\n",
    "        extract_EaglePack_and_process(r\"P:\\datasets\\wallhaven\")\n",
    "        end_time = time.time()  # End timer for entire process\n",
    "        print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n",
    "    finally:\n",
    "        if use_LLM:\n",
    "            llm_processor.stop()\n",
    "    print(\"Processing completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
